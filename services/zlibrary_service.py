# -*- coding: utf-8 -*-
"""
Z-Library æœåŠ¡

åˆ†ç¦»æœç´¢å’Œä¸‹è½½åŠŸèƒ½ï¼Œæä¾›æ›´å¥½çš„é”™è¯¯å¤„ç†ã€‚
"""

import nest_asyncio

nest_asyncio.apply()

import asyncio
import difflib
import json
import os
import random
import re
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests
import zlibrary

from core.pipeline import NetworkError, ProcessingError, ResourceNotFoundError
from utils.logger import get_logger


class ZLibrarySearchService:
    """Z-Libraryæœç´¢æœåŠ¡ - ä¸“é—¨è´Ÿè´£æœç´¢åŠŸèƒ½"""

    def __init__(self,
                 email: str,
                 password: str,
                 proxy_list: List[str] = None,
                 min_delay: float = 1.0,
                 max_delay: float = 3.0):
        """
        åˆå§‹åŒ–æœç´¢æœåŠ¡
        
        Args:
            email: Z-Library è´¦å·
            password: å¯†ç 
            proxy_list: ä»£ç†åˆ—è¡¨
            min_delay: æœ€å°å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            max_delay: æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.logger = get_logger("zlibrary_search")
        self.__email = email
        self.__password = password
        self.proxy_list = proxy_list or []
        self.min_delay = min_delay
        self.max_delay = max_delay

        # é”™è¯¯è®¡æ•°å’Œè¯·æ±‚è®¡æ•°
        self.consecutive_errors = 0
        self.request_count = 0

        # å®¢æˆ·ç«¯å®ä¾‹
        self.lib = None

        # ä¸åœ¨åˆå§‹åŒ–æ—¶ç«‹å³è¿æ¥ï¼Œæ”¹ä¸ºå»¶è¿Ÿè¿æ¥
        # self.ensure_connected()

        # æœç´¢ç­–ç•¥
        self.search_strategies = [{
            'name':
            'ISBNæœç´¢',
            'priority':
            1,
            'build_query':
            self._build_isbn_query,
            'condition':
            lambda t, a, i, p: bool(i and i.strip())
        }, {
            'name':
            'ä¹¦å+ä½œè€…+å‡ºç‰ˆç¤¾æœç´¢',
            'priority':
            2,
            'build_query':
            self._build_full_query,
            'condition':
            lambda t, a, i, p: bool(t and a and p)
        }, {
            'name': 'ä¹¦å+ä½œè€…æœç´¢',
            'priority': 3,
            'build_query': self._build_title_author_query,
            'condition': lambda t, a, i, p: bool(t and a)
        }, {
            'name': 'ä»…ä¹¦åæœç´¢',
            'priority': 4,
            'build_query': self._build_title_query,
            'condition': lambda t, a, i, p: bool(t)
        }]

        self.ensure_connected()

    def ensure_connected(self) -> bool:
        """ç¡®ä¿å®¢æˆ·ç«¯å·²è¿æ¥ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
        max_retries = 3
        base_delay = 2.0

        for attempt in range(1, max_retries + 1):
            try:
                if self.lib is None:
                    self.logger.info(
                        f'å¼€å§‹ç™»é™†Zlibrary (å°è¯• {attempt}/{max_retries})')
                    self.lib = zlibrary.AsyncZlib(proxy_list=self.proxy_list)
                    # Login first - zlibrary will assign personal domain
                    asyncio.run(self.lib.login(self.__email, self.__password))
                    self.logger.info('Zlibraryç™»å½•æˆåŠŸ')
                    # Log the domain assigned after login (should be personal subdomain)
                    self.logger.info(f'Personal domain after login: {self.lib.domain}')
                    self.logger.info(f'Mirror: {self.lib.mirror if hasattr(self.lib, "mirror") else "N/A"}')
                # æ— è®ºæ˜¯æ–°åˆ›å»ºè¿æ¥è¿˜æ˜¯å·²æœ‰è¿æ¥ï¼Œéƒ½åº”è¯¥è¿”å›True
                return True

            except Exception as e:
                error_msg = str(e)
                self.consecutive_errors += 1

                if attempt < max_retries:
                    retry_delay = base_delay * (2**(attempt - 1))  # æŒ‡æ•°é€€é¿
                    self.logger.warning(
                        f"Z-Libraryè¿æ¥å¤±è´¥ï¼Œ{retry_delay}ç§’åé‡è¯•ç¬¬{attempt + 1}æ¬¡: {error_msg}"
                    )
                    time.sleep(retry_delay)
                    # é‡ç½®libä»¥ä¾¿ä¸‹æ¬¡é‡æ–°è¿æ¥
                    self.lib = None
                    continue
                else:
                    self.logger.error(
                        f"Z-Libraryè¿æ¥å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {error_msg}")
                    raise NetworkError(
                        f"Z-Libraryè¿æ¥å¤±è´¥ï¼ˆé‡è¯•{max_retries}æ¬¡åå¤±è´¥ï¼‰: {error_msg}")

        return False

    def search_books(self,
                     title: str = None,
                     author: str = None,
                     isbn: str = None,
                     publisher: str = None) -> List[Dict[str, Any]]:
        """
        æœç´¢ä¹¦ç±
        
        Args:
            title: ä¹¦å
            author: ä½œè€…
            isbn: ISBN
            publisher: å‡ºç‰ˆç¤¾
            
        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœåˆ—è¡¨
        """
        self.logger.info(
            f"å¼€å§‹æ¸è¿›å¼æœç´¢: ä¹¦å='{title}', ä½œè€…='{author}', ISBN='{isbn}', å‡ºç‰ˆç¤¾='{publisher}'"
        )

        # è·å–é€‚ç”¨çš„æœç´¢ç­–ç•¥
        applicable_strategies = self._get_applicable_strategies(
            title, author, isbn, publisher)

        if not applicable_strategies:
            raise ProcessingError("æœç´¢å‚æ•°ä¸è¶³ï¼Œæ²¡æœ‰é€‚ç”¨çš„æœç´¢ç­–ç•¥")

        # æŒ‰ä¼˜å…ˆçº§æ‰§è¡Œæœç´¢
        last_network_error = None

        for strategy in applicable_strategies:
            try:
                results = self._execute_search_strategy(strategy)
                if results:
                    self.consecutive_errors = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                    return results

            except (NetworkError, asyncio.TimeoutError) as e:
                # ç½‘ç»œé”™è¯¯ï¼Œè®°å½•ä½†ç»§ç»­å°è¯•å…¶ä»–ç­–ç•¥
                last_network_error = e
                self.logger.error(f"ç­–ç•¥ {strategy['priority']} ç½‘ç»œé”™è¯¯: {str(e)}")
                self.consecutive_errors += 1
                # ç½‘ç»œé”™è¯¯ä¸éœ€è¦é¢å¤–å»¶è¿Ÿï¼Œ_execute_search_strategyå·²ç»å¤„ç†äº†
                continue

            except (ResourceNotFoundError, ProcessingError) as e:
                # èµ„æºä¸å­˜åœ¨æˆ–å¤„ç†é”™è¯¯ï¼Œç»§ç»­å°è¯•å…¶ä»–ç­–ç•¥
                self.logger.warning(f"ç­–ç•¥ {strategy['priority']} å¤±è´¥: {str(e)}")
                continue

            except Exception as e:
                # å…¶ä»–æœªçŸ¥é”™è¯¯
                traceback.print_exc()
                self.logger.error(
                    f"ç­–ç•¥ {strategy['priority']} å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
                self.consecutive_errors += 1
                self._smart_delay(base_min=3.0,
                                  base_max=6.0,
                                  request_type="error")
                continue

        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥
        if last_network_error:
            # å¦‚æœæœ‰ç½‘ç»œé”™è¯¯ï¼Œä¼˜å…ˆæŠ›å‡ºç½‘ç»œé”™è¯¯
            self.logger.error("æ‰€æœ‰æœç´¢ç­–ç•¥éƒ½å› ç½‘ç»œé”™è¯¯å¤±è´¥")
            raise last_network_error
        else:
            # å¦åˆ™è¡¨ç¤ºæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ç»“æœ
            self.logger.warning("æ‰€æœ‰æœç´¢ç­–ç•¥éƒ½æœªæ‰¾åˆ°ç»“æœ")
            raise ResourceNotFoundError("æœªæ‰¾åˆ°åŒ¹é…çš„ä¹¦ç±")

    def _get_applicable_strategies(self, title: str, author: str, isbn: str,
                                   publisher: str) -> List[Dict[str, Any]]:
        """è·å–é€‚ç”¨çš„æœç´¢ç­–ç•¥"""
        applicable_strategies = []

        for strategy in self.search_strategies:
            if strategy['condition'](title, author, isbn, publisher):
                query = strategy['build_query'](title, author, isbn, publisher)
                applicable_strategies.append({
                    'name': strategy['name'],
                    'priority': strategy['priority'],
                    'query': query
                })

        return applicable_strategies

    async def _async_search_books(self, q, count: int = 10):
        # ç¡®ä¿å·²è¿æ¥å’Œç™»å½•ï¼ˆä½¿ç”¨åŒæ­¥æ–¹æ³•ç¡®ä¿é‡è¯•æœºåˆ¶ï¼‰
        if not self.ensure_connected():
            raise NetworkError("æ— æ³•è¿æ¥åˆ°Z-LibraryæœåŠ¡")

        paginator = await self.lib.search(q=q)
        await paginator.next()

        books_info = []
        for res in paginator.result:
            zlib_id = res.get('id')
            _book = await res.fetch()
            if 'id' not in _book:
                _book['id'] = zlib_id
            books_info.append(_book)
            # # TODO:
            # return books_info
        return books_info

    def _execute_search_strategy(
            self, strategy: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå•ä¸ªæœç´¢ç­–ç•¥ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
        self.logger.info(f"å°è¯•ç­–ç•¥ {strategy['priority']}: {strategy['name']}")
        self.logger.info(f"æœç´¢æŸ¥è¯¢: {strategy['query']}")

        max_retries = 3
        base_delay = 2.0

        for attempt in range(1, max_retries + 1):
            try:
                # ç¡®ä¿è¿æ¥ï¼ˆåœ¨æ¯æ¬¡å°è¯•æ—¶æ£€æŸ¥ï¼‰
                if not self.ensure_connected():
                    raise NetworkError("æ— æ³•è¿æ¥åˆ°Z-LibraryæœåŠ¡")

                # æ™ºèƒ½å»¶è¿Ÿ
                self._smart_delay(request_type="search")
                self.request_count += 1

                # æ‰§è¡Œæœç´¢
                first_set = asyncio.run(
                    self._async_search_books(strategy['query']))

                # æœç´¢æˆåŠŸï¼Œé‡ç½®é”™è¯¯è®¡æ•°
                self.consecutive_errors = 0
                break

            except Exception as e:
                error_msg = str(e)
                is_connection_reset = ("Connection reset by peer" in error_msg
                                       or "[Errno 54]" in error_msg
                                       or "ClientOSError" in str(type(e)))

                if is_connection_reset and attempt < max_retries:
                    # è¿æ¥é‡ç½®é”™è¯¯ï¼Œå¯ä»¥é‡è¯•
                    retry_delay = base_delay * (2**(attempt - 1))  # æŒ‡æ•°é€€é¿
                    self.logger.warning(
                        f"é‡åˆ°è¿æ¥é‡ç½®é”™è¯¯ï¼Œ{retry_delay}ç§’åè¿›è¡Œç¬¬{attempt + 1}æ¬¡å°è¯•: {error_msg}"
                    )
                    self.consecutive_errors += 1
                    time.sleep(retry_delay)
                    continue
                elif is_connection_reset:
                    # é‡è¯•æ¬¡æ•°ç”¨å®Œï¼ŒæŠ›å‡ºç½‘ç»œé”™è¯¯
                    self.logger.error(
                        f"è¿æ¥é‡ç½®é”™è¯¯é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {error_msg}")
                    raise NetworkError(
                        f"è¿æ¥é‡ç½®é”™è¯¯ï¼ˆé‡è¯•{max_retries}æ¬¡åå¤±è´¥ï¼‰: {error_msg}")
                else:
                    # å…¶ä»–ç±»å‹çš„é”™è¯¯ç›´æ¥æŠ›å‡º
                    self.logger.error(f"æœç´¢è¿‡ç¨‹ä¸­é‡åˆ°éç½‘ç»œé”™è¯¯: {error_msg}")
                    raise
        else:
            # forå¾ªç¯æ²¡æœ‰breakï¼Œè¯´æ˜é‡è¯•ç”¨å®Œäº†
            raise NetworkError("æœç´¢é‡è¯•æ¬¡æ•°ç”¨å®Œ")

        if not first_set:
            self.logger.info(f"æœç´¢æ— ç»“æœ")
            return []

        # å¤„ç†ç»“æœ
        processed_results = self._process_search_results(first_set)

        self.logger.info(
            f"ç­–ç•¥ '{strategy['name']}' æ‰¾åˆ° {len(processed_results)} ä¸ªç»“æœ")

        return processed_results

    def _process_authors(self, author_info):
        if isinstance(author_info, str):
            return author_info
        if isinstance(author_info, list):
            return ";;".join(
                [self._process_authors(_info) for _info in author_info])
        if isinstance(author_info, dict):
            return author_info['author']

    def _process_search_results(self,
                                results: List[Any]) -> List[Dict[str, Any]]:
        """å¤„ç†æœç´¢ç»“æœ"""
        processed_results = []

        for i, result in enumerate(results):
            # æå–ä¹¦ç±ä¿¡æ¯
            book_info = {
                'zlibrary_id': result.get('id'),
                'title': result.get('name'),
                'authors': self._process_authors(result.get('authors', '')),
                'extension': result.get('extension', '').lower(),
                'size': result.get('size'),
                'isbn': result.get('isbn', ''),
                'url': result.get('url', ''),
                'cover': result.get('cover', ''),
                'description': result.get('description', ''),
                'edition': result.get('edition', ''),
                'categories': result.get('categories', ''),
                'categories_url': result.get('categories_url', ''),
                'download_url': result.get('download_url', ''),
                'publisher': result.get('publisher', ''),
                'year': result.get('year', ''),
                'language': result.get('language', ''),
                'rating': result.get('rating', ''),
                'quality': result.get('quality', ''),
                'raw_json': json.dumps(result, ensure_ascii=False)
            }

            processed_results.append(book_info)

        return processed_results

    def calculate_match_score(self, douban_book: Dict[str, str],
                              zlibrary_book: Dict[str, str]) -> float:
        """
        è®¡ç®—è±†ç“£ä¹¦ç±å’ŒZ-Libraryä¹¦ç±çš„åŒ¹é…åº¦å¾—åˆ†
        
        Args:
            douban_book: è±†ç“£ä¹¦ç±ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«title, author, publisher, yearç­‰
            zlibrary_book: Z-Libraryä¹¦ç±ä¿¡æ¯å­—å…¸
            
        Returns:
            float: åŒ¹é…åº¦å¾—åˆ†ï¼ŒèŒƒå›´0.0-1.0
        """
        score = 0.0

        # 1. ä¹¦åç›¸ä¼¼åº¦ (æƒé‡: 0.4)
        title_score = self._calculate_text_similarity(
            douban_book.get('title', ''), zlibrary_book.get('title', ''))
        score += title_score * 0.4

        # 2. ä½œè€…ç›¸ä¼¼åº¦ (æƒé‡: 0.3)
        douban_author = douban_book.get('author', '')
        zlibrary_authors = zlibrary_book.get('authors', '').replace(';;', ' ')
        author_score = self._calculate_text_similarity(douban_author,
                                                       zlibrary_authors)
        score += author_score * 0.3

        # 3. å‡ºç‰ˆç¤¾ç›¸ä¼¼åº¦ (æƒé‡: 0.15)
        publisher_score = self._calculate_text_similarity(
            douban_book.get('publisher', ''),
            zlibrary_book.get('publisher', ''))
        score += publisher_score * 0.15

        # 4. å¹´ä»½åŒ¹é… (æƒé‡: 0.1)
        year_score = self._calculate_year_similarity(
            douban_book.get('publish_date', ''), zlibrary_book.get('year', ''))
        score += year_score * 0.1

        # 5. ISBNå®Œå…¨åŒ¹é…å¥–åŠ± (æƒé‡: 0.05)
        isbn_score = self._calculate_isbn_similarity(
            douban_book.get('isbn', ''), zlibrary_book.get('isbn', ''))
        score += isbn_score * 0.05

        return min(1.0, score)  # ç¡®ä¿ä¸è¶…è¿‡1.0

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0

        # é¢„å¤„ç†ï¼šè½¬æ¢ä¸ºå°å†™ï¼Œç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œå¤šä½™ç©ºæ ¼
        text1 = re.sub(r'[^\w\s]', ' ', text1.lower()).strip()
        text2 = re.sub(r'[^\w\s]', ' ', text2.lower()).strip()

        if text1 == text2:
            return 1.0

        # ä½¿ç”¨difflibè®¡ç®—åºåˆ—ç›¸ä¼¼åº¦
        similarity = difflib.SequenceMatcher(None, text1, text2).ratio()
        return similarity

    def _calculate_year_similarity(self, date_str: str,
                                   year_str: str) -> float:
        """è®¡ç®—å¹´ä»½ç›¸ä¼¼åº¦"""
        if not date_str or not year_str:
            return 0.0

        try:
            # ä»æ—¥æœŸå­—ç¬¦ä¸²ä¸­æå–å¹´ä»½
            douban_year = re.search(r'\d{4}', date_str)
            if douban_year:
                douban_year = int(douban_year.group())
                zlibrary_year = int(year_str)

                # å¹´ä»½å®Œå…¨åŒ¹é…
                if douban_year == zlibrary_year:
                    return 1.0
                # å¹´ä»½ç›¸å·®1å¹´å†…
                elif abs(douban_year - zlibrary_year) <= 1:
                    return 0.8
                # å¹´ä»½ç›¸å·®2å¹´å†…
                elif abs(douban_year - zlibrary_year) <= 2:
                    return 0.6
                else:
                    return 0.0
        except (ValueError, AttributeError):
            return 0.0

        return 0.0

    def _calculate_isbn_similarity(self, isbn1: str, isbn2: str) -> float:
        """è®¡ç®—ISBNç›¸ä¼¼åº¦"""
        if not isbn1 or not isbn2:
            return 0.0

        # ç§»é™¤ISBNä¸­çš„éæ•°å­—å­—ç¬¦
        isbn1_clean = re.sub(r'[^\d]', '', isbn1)
        isbn2_clean = re.sub(r'[^\d]', '', isbn2)

        if isbn1_clean and isbn2_clean and isbn1_clean == isbn2_clean:
            return 1.0

        return 0.0

    def _build_isbn_query(self,
                          title: str = None,
                          author: str = None,
                          isbn: str = None,
                          publisher: str = None) -> str:
        """æ„å»ºISBNæœç´¢æŸ¥è¯¢ - Plain ISBN works, 'isbn:' prefix doesn't!"""
        return isbn.strip()  # Just the ISBN number, no prefix!

    def _build_full_query(self,
                          title: str = None,
                          author: str = None,
                          isbn: str = None,
                          publisher: str = None) -> str:
        """æ„å»ºä¹¦å+ä½œè€…+å‡ºç‰ˆç¤¾æœç´¢æŸ¥è¯¢"""
        parts = [title.strip(), author.strip(), publisher.strip()]
        return ' '.join(parts)

    def _build_title_author_query(self,
                                  title: str = None,
                                  author: str = None,
                                  isbn: str = None,
                                  publisher: str = None) -> str:
        """æ„å»ºä¹¦å+ä½œè€…æœç´¢æŸ¥è¯¢"""
        parts = [title.strip(), author.strip()]
        return ' '.join(parts)

    def _build_title_query(self,
                           title: str = None,
                           author: str = None,
                           isbn: str = None,
                           publisher: str = None) -> str:
        """æ„å»ºä»…ä¹¦åæœç´¢æŸ¥è¯¢"""
        return title.strip()

    def _smart_delay(self,
                     base_min: float = None,
                     base_max: float = None,
                     request_type: str = "normal"):
        """æ™ºèƒ½å»¶è¿Ÿ"""
        min_delay = base_min or self.min_delay
        max_delay = base_max or self.max_delay

        # æ ¹æ®è¯·æ±‚ç±»å‹è°ƒæ•´å»¶è¿Ÿ
        if request_type == "search":
            min_delay = max(min_delay * 1.5, 2.0)
            max_delay = max(max_delay * 1.5, 4.0)
        elif request_type == "error":
            min_delay = max(min_delay * 2, 3.0)
            max_delay = max(max_delay * 2, 6.0)

        # æ ¹æ®è¿ç»­é”™è¯¯å¢åŠ å»¶è¿Ÿ
        if self.consecutive_errors > 0:
            error_multiplier = min(1.5**self.consecutive_errors, 4.0)
            min_delay *= error_multiplier
            max_delay *= error_multiplier

        # æ‰§è¡Œå»¶è¿Ÿ
        delay = random.uniform(min_delay, max_delay)
        self.logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’")
        time.sleep(delay)


class ZLibraryDownloadService:
    """Z-Libraryä¸‹è½½æœåŠ¡ - ä¸“é—¨è´Ÿè´£ä¸‹è½½åŠŸèƒ½"""

    def __init__(self,
                 email: str,
                 password: str,
                 proxy_list: List[str] = None,
                 format_priority: List[str] = None,
                 min_delay: float = 2.0,
                 max_delay: float = 5.0,
                 max_retries: int = 3):
        """
        åˆå§‹åŒ–ä¸‹è½½æœåŠ¡
        
        Args:
            email: Z-Library è´¦å·
            password: å¯†ç 
            proxy_list: ä»£ç†åˆ—è¡¨
            format_priority: æ ¼å¼ä¼˜å…ˆçº§
            min_delay: æœ€å°å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            max_delay: æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.logger = get_logger("zlibrary_download")
        self.__email = email
        self.__password = password
        self.proxy_list = proxy_list or []
        self.format_priority = format_priority or ['epub', 'mobi', 'pdf']
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.max_retries = max_retries

        # é”™è¯¯è®¡æ•°
        self.consecutive_errors = 0
        self.request_count = 0

        # å®¢æˆ·ç«¯å®ä¾‹
        self.lib = None

    def ensure_connected(self) -> bool:
        """ç¡®ä¿å®¢æˆ·ç«¯å·²è¿æ¥ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
        max_retries = 3
        base_delay = 2.0

        for attempt in range(1, max_retries + 1):
            try:
                if self.lib is None:
                    self.logger.info(
                        f'å¼€å§‹ç™»é™†Zlibrary (å°è¯• {attempt}/{max_retries})')
                    self.lib = zlibrary.AsyncZlib(proxy_list=self.proxy_list)
                    # Login first - zlibrary will assign personal domain
                    asyncio.run(self.lib.login(self.__email, self.__password))
                    self.logger.info('Zlibraryç™»å½•æˆåŠŸ')
                    # Log the domain assigned after login
                    self.logger.info(f'Personal domain after login: {self.lib.domain}')
                # æ— è®ºæ˜¯æ–°åˆ›å»ºè¿æ¥è¿˜æ˜¯å·²æœ‰è¿æ¥ï¼Œéƒ½åº”è¯¥è¿”å›True
                return True

            except Exception as e:
                error_msg = str(e)
                self.consecutive_errors += 1

                if attempt < max_retries:
                    retry_delay = base_delay * (2**(attempt - 1))  # æŒ‡æ•°é€€é¿
                    self.logger.warning(
                        f"Z-Libraryè¿æ¥å¤±è´¥ï¼Œ{retry_delay}ç§’åé‡è¯•ç¬¬{attempt + 1}æ¬¡: {error_msg}"
                    )
                    time.sleep(retry_delay)
                    # é‡ç½®libä»¥ä¾¿ä¸‹æ¬¡é‡æ–°è¿æ¥
                    self.lib = None
                    continue
                else:
                    self.logger.error(
                        f"Z-Libraryè¿æ¥å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {error_msg}")
                    raise NetworkError(
                        f"Z-Libraryè¿æ¥å¤±è´¥ï¼ˆé‡è¯•{max_retries}æ¬¡åå¤±è´¥ï¼‰: {error_msg}")

        return False

    def download_book(self, book_info: Dict[str, Any],
                      output_dir: str) -> Optional[str]:
        """
        ä¸‹è½½ä¹¦ç±æ–‡ä»¶
        
        Args:
            book_info: ä¹¦ç±ä¿¡æ¯
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Optional[str]: ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„
        """
        self.ensure_connected()

        output_path = Path(output_dir)
        os.makedirs(output_path, exist_ok=True)

        # æš‚æ—¶æ„å»ºé»˜è®¤æ–‡ä»¶åï¼ˆå¦‚æœå“åº”å¤´ä¸­æ²¡æœ‰æ–‡ä»¶åä¼šä½¿ç”¨è¿™ä¸ªï¼‰
        title = book_info.get('title', 'Unknown')
        authors = book_info.get('authors', 'Unknown')
        extension = book_info.get('extension', 'epub')

        # å¤„ç†ä½œè€…å­—æ®µ
        if ';;' in authors:
            author = authors.split(';;')[0]  # å–ç¬¬ä¸€ä¸ªä½œè€…
        else:
            author = authors

        default_file_name = f"{title} - {author}.{extension}"
        default_file_name = self._sanitize_filename(default_file_name)

        self.logger.info(f"å¼€å§‹ä¸‹è½½: {title}")

        # # è·å–ä¹¦ç±ID
        # book_id = book_info.get('zlibrary_id') or book_info.get('id')

        # # å¦‚æœIDä¸ºç©ºæˆ–ä¸º'None'å­—ç¬¦ä¸²ï¼Œå°è¯•ä»type:download_urlä¸­æå–
        # if not book_id or book_id == 'None':
        #     download_url = book_info.get('download_url', '')
        #     if download_url:
        #         # ä» URL ä¸­æå– IDï¼Œæ ¼å¼ç±»ä¼¼ https://z-library.sk/dl/25295952/7c99fd
        #         import re
        #         match = re.search(r'/dl/(\d+)/', download_url)
        #         if match:
        #             book_id = match.group(1)
        #             self.logger.info(f"ä»ä¸‹è½½é“¾æ¥æå–åˆ°ID: {book_id}")

        # if not book_id or book_id == 'None':
        #     raise ProcessingError("ä¹¦ç±ä¿¡æ¯ä¸­ç¼ºå°‘IDï¼Œä¸”æ— æ³•ä»ä¸‹è½½é“¾æ¥æå–")

        # æ‰§è¡Œä¸‹è½½ï¼Œæ”¯æŒé‡è¯•
        for attempt in range(1, self.max_retries + 1):
            try:
                self.logger.info(f"ä¸‹è½½å°è¯• {title} {attempt}/{self.max_retries}")

                # æ™ºèƒ½å»¶è¿Ÿ
                self._smart_delay(request_type="download")
                self.request_count += 1

                # è·å–ä¸‹è½½é“¾æ¥ï¼ˆä¼˜å…ˆä½¿ç”¨book_infoä¸­çš„ï¼Œå¦åˆ™ä½¿ç”¨zlibrary APIè·å–ï¼‰
                download_url = book_info.get('download_url')
                if not download_url:
                    self.logger.info(f"æœªæ‰¾åˆ° {title} ç›´æ¥ä¸‹è½½é“¾æ¥ï¼Œå°è¯•ä½¿ç”¨Z-Library APIè·å–")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ é€šè¿‡zlibrary APIè·å–ä¸‹è½½é“¾æ¥çš„é€»è¾‘
                    raise ProcessingError(f"{title} ä¹¦ç±ä¿¡æ¯ä¸­ç¼ºå°‘ä¸‹è½½é“¾æ¥")

                # ä½¿ç”¨requestsä¸‹è½½æ–‡ä»¶
                headers = {
                    'User-Agent':
                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    # 'Referer':
                    # 'https://z-library.sk/',
                    # 'Accept':
                    # 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
                }

                self.logger.info(f"ä½¿ç”¨é“¾æ¥ä¸‹è½½: {download_url}")

                # Use cookies from book_info if provided (from discord bot)
                # Otherwise use self.lib.cookies
                cookies_to_use = book_info.get('cookies', self.lib.cookies if self.lib else {})
                
                if cookies_to_use:
                    headers['Cookie'] = "; ".join(
                        [f"{k}={v}" for k, v in cookies_to_use.items()] +
                        ["switchLanguage=zh", "siteLanguage=zh"])
                    self.logger.info(f"Using {len(cookies_to_use)} cookies for authenticated download")
                else:
                    self.logger.warning("No cookies available, download may fail")

                # é…ç½®ä»£ç†
                proxies = None
                if self.proxy_list:
                    proxy_url = random.choice(self.proxy_list)
                    proxies = {'http': proxy_url, 'https': proxy_url}
                    self.logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_url}")

                # ä½¿ç”¨ AsyncZlib çš„ cookies
                # cookies = None
                # if self.lib and hasattr(self.lib,
                #                         'cookies') and self.lib.cookies:
                #     try:
                #         # å°† aiohttp cookies è½¬æ¢ä¸º requests å¯ç”¨çš„æ ¼å¼
                #         cookies = {}
                #         if hasattr(self.lib.cookies, '_cookies'):
                #             for domain_cookies in self.lib.cookies._cookies.values(
                #             ):
                #                 for path_cookies in domain_cookies.values():
                #                     for cookie in path_cookies.values():
                #                         cookies[cookie.key] = cookie.value
                #         self.logger.info(
                #             f"ä½¿ç”¨ AsyncZlib cookiesï¼Œå…± {len(cookies)} ä¸ª")
                #     except Exception as e:
                #         self.logger.warning(
                #             f"è·å– AsyncZlib cookies å¤±è´¥: {str(e)}")
                #         cookies = None

                response = requests.get(
                    download_url,
                    headers=headers,
                    # cookies=cookies,
                    proxies=proxies,
                    stream=True,
                    timeout=30)

                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code != 200:
                    raise ProcessingError(
                        f"ä¸‹è½½å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")

                # æ£€æŸ¥å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '')
                self.logger.info(f"å“åº”å†…å®¹ç±»å‹: {content_type}")

                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                content_length = response.headers.get('content-length')
                if content_length:
                    self.logger.info(f"æ–‡ä»¶å¤§å°: {int(content_length):,} bytes")

                # å°è¯•ä»å“åº”å¤´è·å–åŸå§‹æ–‡ä»¶å
                content_disposition = response.headers.get(
                    'content-disposition', '')
                original_filename = self._extract_filename_from_content_disposition(
                    content_disposition)

                if original_filename:
                    self.logger.info(f"ä½¿ç”¨å“åº”å¤´ä¸­çš„åŸå§‹æ–‡ä»¶å: {original_filename}")
                    file_name = self._sanitize_filename(original_filename)
                else:
                    self.logger.info(f"å“åº”å¤´ä¸­æ²¡æœ‰æ–‡ä»¶åï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶å: {default_file_name}")
                    file_name = default_file_name

                file_path = output_path / file_name

                # ä¿å­˜æ–‡ä»¶ - vá»›i progress bar
                downloaded_size = 0
                total_size = int(response.headers.get('content-length', 0))
                
                # Print initial progress
                if total_size > 0:
                    print(f"\nğŸ“¥ Downloading: {file_name}")
                    print(f"ğŸ“¦ Total size: {total_size / (1024*1024):.2f} MB")
                
                with open(str(file_path), 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded_size += len(chunk)
                            
                            # Print progress every 1MB
                            if total_size > 0 and downloaded_size % (1024 * 1024) < 8192:
                                percent = (downloaded_size / total_size) * 100
                                downloaded_mb = downloaded_size / (1024 * 1024)
                                total_mb = total_size / (1024 * 1024)
                                # Print with carriage return to overwrite previous line
                                print(f"\râ¬‡ï¸  Progress: {percent:.1f}% ({downloaded_mb:.1f}/{total_mb:.1f} MB)", end='', flush=True)
                
                # Final newline after progress
                if total_size > 0:
                    print()  # New line after progress bar
                
                self.logger.info(f"ä¸‹è½½å®Œæˆï¼Œå®é™…å¤§å°: {downloaded_size:,} bytes")

                self.consecutive_errors = 0
                self.logger.info(f"ä¸‹è½½æˆåŠŸ: {file_path}")
                return str(file_path)

            except Exception as e:
                # traceback.print_exc()
                error_msg = str(e)
                is_connection_reset = ("Connection reset by peer" in error_msg
                                       or "[Errno 54]" in error_msg
                                       or "ClientOSError" in str(type(e)))

                self.consecutive_errors += 1

                if is_connection_reset:
                    self.logger.warning(
                        f"ä¸‹è½½å°è¯• {attempt} é‡åˆ°è¿æ¥é‡ç½®é”™è¯¯: {error_msg}")
                else:
                    self.logger.error(f"ä¸‹è½½å°è¯• {attempt} å¤±è´¥: {error_msg}")

                if attempt < self.max_retries:
                    # æ ¹æ®é”™è¯¯ç±»å‹é€‰æ‹©å»¶è¿Ÿæ—¶é—´
                    if is_connection_reset:
                        # è¿æ¥é‡ç½®é”™è¯¯ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
                        retry_delay = 2.0 * (2**(attempt - 1))
                        self.logger.info(f"è¿æ¥é‡ç½®é”™è¯¯ï¼Œ{retry_delay}ç§’åé‡è¯•")
                        time.sleep(retry_delay)
                    else:
                        # å…¶ä»–é”™è¯¯ï¼Œä½¿ç”¨åŸæœ‰å»¶è¿Ÿ
                        self._smart_delay(base_min=5.0,
                                          base_max=10.0,
                                          request_type="error")
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œåˆ¤æ–­é”™è¯¯ç±»å‹
                    if is_connection_reset:
                        raise NetworkError(
                            f"è¿æ¥é‡ç½®é”™è¯¯ï¼ˆé‡è¯•{self.max_retries}æ¬¡åå¤±è´¥ï¼‰: {error_msg}")
                    elif "not found" in error_msg.lower(
                    ) or "404" in error_msg:
                        raise ResourceNotFoundError(f"ä¹¦ç±æ–‡ä»¶ä¸å­˜åœ¨: {error_msg}")
                    else:
                        raise ProcessingError(f"ä¸‹è½½å¤±è´¥: {error_msg}")

        return None

    # å·²åˆ é™¤ _save_download_resultï¼Œç›´æ¥ä½¿ç”¨requestsä¸‹è½½

    def _extract_filename_from_content_disposition(
            self, content_disposition: str) -> Optional[str]:
        """ä» Content-Disposition å¤´ä¸­æå–æ–‡ä»¶å"""
        if not content_disposition:
            return None

        # åŒ¹é… filename="xxx" æˆ– filename*=UTF-8''xxx æ ¼å¼
        import re

        # é¦–å…ˆå°è¯•åŒ¹é… filename*=UTF-8''xxx æ ¼å¼ï¼ˆRFC 5987ï¼‰
        match = re.search(r"filename\*=UTF-8''([^;]+)", content_disposition,
                          re.IGNORECASE)
        if match:
            import urllib.parse
            try:
                filename = urllib.parse.unquote(match.group(1))
                return filename
            except:
                pass

        # ç„¶åå°è¯•åŒ¹é… filename="xxx" æˆ– filename=xxx æ ¼å¼
        match = re.search(r'filename[^;=\n]*=(([\'"])([^\'"]*?)\2|([^;\n]*))',
                          content_disposition, re.IGNORECASE)
        if match:
            filename = match.group(3) or match.group(4)
            if filename:
                return filename.strip()

        return None

    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        illegal_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
        for char in illegal_chars:
            filename = filename.replace(char, '_')

        # é™åˆ¶é•¿åº¦
        if len(filename) > 200:
            name_parts = filename.split('.')
            extension = name_parts[-1] if len(name_parts) > 1 else ''
            base_name = '.'.join(
                name_parts[:-1]) if len(name_parts) > 1 else filename
            base_name = base_name[:200 - len(extension) - 1]
            filename = f"{base_name}.{extension}" if extension else base_name

        return filename

    def _smart_delay(self,
                     base_min: float = None,
                     base_max: float = None,
                     request_type: str = "normal"):
        """æ™ºèƒ½å»¶è¿Ÿ"""
        min_delay = base_min or self.min_delay
        max_delay = base_max or self.max_delay

        # æ ¹æ®è¯·æ±‚ç±»å‹è°ƒæ•´å»¶è¿Ÿ
        if request_type == "download":
            min_delay = max(min_delay * 1.5, 3.0)
            max_delay = max(max_delay * 1.5, 6.0)
        elif request_type == "error":
            min_delay = max(min_delay * 2, 5.0)
            max_delay = max(max_delay * 2, 10.0)

        # æ ¹æ®è¿ç»­é”™è¯¯å¢åŠ å»¶è¿Ÿ
        if self.consecutive_errors > 0:
            error_multiplier = min(1.5**self.consecutive_errors, 4.0)
            min_delay *= error_multiplier
            max_delay *= error_multiplier

        # æ‰§è¡Œå»¶è¿Ÿ
        delay = random.uniform(min_delay, max_delay)
        self.logger.debug(f"å»¶è¿Ÿ {delay:.2f} ç§’")
        time.sleep(delay)


class ZLibraryService:
    """Z-Library æœåŠ¡ - æ•´åˆæœç´¢å’Œä¸‹è½½æœåŠ¡"""

    def __init__(self,
                 email: str,
                 password: str,
                 proxy_list: List[str] = None,
                 format_priority: List[str] = None,
                 download_dir: str = "data/downloads"):
        """
        åˆå§‹åŒ–Z-LibraryæœåŠ¡
        
        Args:
            email: Z-Library è´¦å·
            password: å¯†ç 
            proxy_list: ä»£ç†åˆ—è¡¨
            format_priority: æ ¼å¼ä¼˜å…ˆçº§
            download_dir: ä¸‹è½½ç›®å½•
        """
        self.logger = get_logger("zlibrary_service")

        # åˆå§‹åŒ–å­æœåŠ¡
        self.search_service = ZLibrarySearchService(email=email,
                                                    password=password,
                                                    proxy_list=proxy_list)

        self.download_service = ZLibraryDownloadService(
            email=email,
            password=password,
            proxy_list=proxy_list,
            format_priority=format_priority)

        self.download_dir = download_dir

    def search_books(self,
                     title: str = None,
                     author: str = None,
                     isbn: str = None,
                     publisher: str = None) -> List[Dict[str, Any]]:
        """
        æœç´¢ä¹¦ç±
        
        Args:
            title: ä¹¦å
            author: ä½œè€…
            isbn: ISBN
            publisher: å‡ºç‰ˆç¤¾
            
        Returns:
            List[Dict[str, Any]]: æœç´¢ç»“æœåˆ—è¡¨
        """
        return self.search_service.search_books(title=title,
                                                author=author,
                                                isbn=isbn,
                                                publisher=publisher)

    def download_book(self,
                      book_info: Dict[str, Any],
                      output_dir: str = None) -> Optional[str]:
        """
        ä¸‹è½½ä¹¦ç±æ–‡ä»¶
        
        Args:
            book_info: ä¹¦ç±ä¿¡æ¯
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Optional[str]: ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„
        """
        if output_dir is None:
            output_dir = self.download_dir

        return self.download_service.download_book(book_info, output_dir)

    def get_download_limits(self) -> Dict[str, int]:
        """
        è·å–Z-Libraryä¸‹è½½é™åˆ¶ä¿¡æ¯
        
        Returns:
            Dict[str, int]: åŒ…å«ä¸‹è½½é™åˆ¶ä¿¡æ¯çš„å­—å…¸
                - daily_amount: æ¯æ—¥æ€»é™é¢
                - daily_allowed: æ¯æ—¥å…è®¸ä¸‹è½½
                - daily_remaining: æ¯æ—¥å‰©ä½™ä¸‹è½½æ¬¡æ•° 
                - daily_reset: ä¸‹æ¬¡é‡ç½®æ—¶é—´æˆ³
        """
        try:
            # ç¡®ä¿ä¸‹è½½æœåŠ¡è¿æ¥
            self.download_service.ensure_connected()

            # è·å–é™åˆ¶ä¿¡æ¯
            limits = asyncio.run(
                self.download_service.lib.profile.get_limits())

            self.logger.info(f"è·å–ä¸‹è½½é™åˆ¶: {limits}")

            return limits

        except Exception as e:
            self.logger.error(f"è·å–ä¸‹è½½é™åˆ¶å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤å€¼ï¼Œé¿å…é˜»å¡æµç¨‹
            return {
                'daily_amount': 0,
                'daily_allowed': 0,
                'daily_remaining': 0,
                'daily_reset': 0
            }

    async def get_download_quota(self) -> Dict[str, Any]:
        """
        å¼‚æ­¥è·å–ä¸‹è½½é…é¢ä¿¡æ¯ï¼ˆä¾›QuotaManagerä½¿ç”¨ï¼‰
        
        Returns:
            Dict[str, Any]: é…é¢ä¿¡æ¯å­—å…¸
                - remaining: å‰©ä½™ä¸‹è½½æ¬¡æ•°
                - daily_limit: æ¯æ—¥é™åˆ¶
                - next_reset: ä¸‹æ¬¡é‡ç½®æ—¶é—´
        """
        try:
            limits = self.get_download_limits()
            return {
                'remaining': limits.get('daily_remaining', 0),
                'daily_limit': limits.get('daily_amount', 10),
                'next_reset': limits.get('daily_reset', None)
            }
        except Exception as e:
            self.logger.error(f"è·å–é…é¢ä¿¡æ¯å¤±è´¥: {e}")
            from core.pipeline import NetworkError
            raise NetworkError(f"æ— æ³•è·å–é…é¢ä¿¡æ¯: {e}")

    def check_download_available(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ä¸‹è½½æ¬¡æ•°

        Returns:
            bool: æ˜¯å¦å¯ä»¥ä¸‹è½½
        """
        try:
            # ç¡®ä¿ä¸‹è½½æœåŠ¡å·²è¿æ¥
            if not self.download_service.ensure_connected():
                self.logger.error("æ— æ³•è¿æ¥åˆ°Z-Libraryä¸‹è½½æœåŠ¡")
                return False

            limits = self.get_download_limits()
            remaining = limits.get('daily_remaining', 0)

            self.logger.info(f"ä¸‹è½½é™åˆ¶æ£€æŸ¥: å‰©ä½™æ¬¡æ•° {remaining}")

            return remaining > 0
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ä¸‹è½½å¯ç”¨æ€§å¤±è´¥: {str(e)}")
            # å‡ºç°å¼‚å¸¸æ—¶å‡è®¾å¯ä»¥ä¸‹è½½ï¼Œé¿å…é˜»å¡æµç¨‹
            return True

    # å·²åˆ é™¤ search_and_download æ–¹æ³•ï¼Œå®Œå…¨åˆ†ç¦» search å’Œ download æ­¥éª¤

    def calculate_match_score(self, douban_book: Dict[str, str],
                              zlibrary_book: Dict[str, str]) -> float:
        """
        è®¡ç®—è±†ç“£ä¹¦ç±å’ŒZ-Libraryä¹¦ç±çš„åŒ¹é…åº¦å¾—åˆ†
        
        Args:
            douban_book: è±†ç“£ä¹¦ç±ä¿¡æ¯å­—å…¸
            zlibrary_book: Z-Libraryä¹¦ç±ä¿¡æ¯å­—å…¸
            
        Returns:
            float: åŒ¹é…åº¦å¾—åˆ†ï¼ŒèŒƒå›´0.0-1.0
        """
        return self.search_service.calculate_match_score(
            douban_book, zlibrary_book)
